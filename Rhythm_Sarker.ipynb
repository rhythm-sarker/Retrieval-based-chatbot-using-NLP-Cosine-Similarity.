{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fKftUZWBYVsP"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "import string\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "\n",
        "f=open('dataset2.txt','r')\n",
        "raw_data=f.read()\n",
        "raw_data=raw_data.lower() #Lower the data\n",
        "sent_token = sent_tokenize(raw_data) #Converts into list of sentences\n",
        "word_token = word_tokenize(raw_data) #Converts into list of words\n",
        "\n",
        "lemmer= nltk.stem.WordNetLemmatizer()\n",
        "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
        "punct=(word_tokenize(string.punctuation))\n",
        "#punctuations\n",
        "\n",
        "def lemtokens(tokens):\n",
        "\treturn[lemmer.lemmatize(token) for token in tokens]\n",
        "\n",
        "remove_punct_dict = dict((ord(punct),None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "\treturn lemtokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
        "#remove punctuations and convert to the normal tokens.\n",
        "\n",
        "lemmer= nltk.stem.WordNetLemmatizer()\n",
        "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
        "punct=(word_tokenize(string.punctuation))\n",
        "#punctuations\n",
        "\n",
        "def lemtokens(tokens):\n",
        "\treturn[lemmer.lemmatize(token) for token in tokens]\n",
        "\n",
        "remove_punct_dict = dict((ord(punct),None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "\treturn lemtokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
        "#remove punctuations and convert to the normal tokens.\n",
        "\n",
        "def response(user_response):\n",
        "\tf=open('dataset2.txt','r')\n",
        "\traw=f.read()\n",
        "\traw=raw.lower()\n",
        "\n",
        "\tsent_token=sent_tokenize(raw)\n",
        "\tword_token=word_tokenize(raw)\n",
        "\n",
        "\tchatbot_resp=\"\"\n",
        "\tsent_token.append(user_response)\n",
        "\n",
        "\tTfidVec=TfidfVectorizer(tokenizer=LemNormalize,stop_words=\"english\")\n",
        "\ttfidf=TfidVec.fit_transform(sent_token) #dataset + user question\n",
        "\tvals=cosine_similarity(tfidf[-1],tfidf) #inputs - userquestion, modified dataset\n",
        "\tidx=vals.argsort()[0][-2]\n",
        "\tflat=vals.flatten()\n",
        "\tflat.sort()\n",
        "\treq_tfidf=flat[-2]\n",
        "\tprint(\"req_tfidf: \",req_tfidf)\n",
        "\tif(req_tfidf==0):\n",
        "\t\tchatbot_resp+=\"Sorry!\"\n",
        "\t\treturn chatbot_resp\n",
        "\telse:\n",
        "\t\tchatbot_resp+=sent_token[idx]\n",
        "\t\treturn chatbot_resp\n",
        "\n",
        "res = response(\"What is global warming?\") #user_input / user question\n",
        "print(\"Chatbot Response : = \",res)\n",
        "\n",
        "def response1(user_response):\n",
        "    # Append user response to sentences\n",
        "    sent_token.append(user_response.lower())\n",
        "\n",
        "    # Create TF-IDF vectors\n",
        "    TfidVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words=\"english\")\n",
        "    tfidf = TfidVec.fit_transform(sent_token)  # Dataset + user question\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf[:-1])  # Compare user question with dataset\n",
        "    idx = vals.argsort()[0][-1]  # Get index of most similar sentence\n",
        "\n",
        "    # Check similarity\n",
        "    req_tfidf = vals.flatten()[idx]\n",
        "    print(\"req_tfidf: \", req_tfidf)\n",
        "\n",
        "    if req_tfidf == 0:\n",
        "        return \"Sorry!\"\n",
        "    else:\n",
        "        return sent_token[idx]\n",
        "\n"
      ],
      "metadata": {
        "id": "IgUj0K72YpTo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcc7d41e-72d9-432e-8b0e-90b0243609a4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "req_tfidf:  0.25140101180980173\n",
            "Chatbot Response : =  addressing climate change requires global cooperation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the chatbot response\n",
        "res = response1(\"What is global warming?\")\n",
        "print(\"Chatbot Response: \", res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFkGQKHDbmnV",
        "outputId": "23f5b1e4-d041-4bb1-e520-37e18cd369cd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "req_tfidf:  0.25140101180980173\n",
            "Chatbot Response:  addressing climate change requires global cooperation.\n"
          ]
        }
      ]
    }
  ]
}